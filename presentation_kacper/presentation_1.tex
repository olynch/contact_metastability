%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amsmath} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{xcolor}
\newcommand{\norm}[1]{{\left\lVert#1\right\rVert}}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Metastability for the Contact Process]{Metastability for the Contact Process on Z: Part 2} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Kacper Urba≈Ñski} % Your name
\institute[UvA] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Universiteit van Amsterdam \\ % Your institution for the title page
\medskip
\textit{kacper.urbanski@protonmail.com} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{Forumlating the theorem} % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
%------------------------------------------------

\subsection{Natural language definition} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks

\begin{frame}
    \frametitle{A bit of terminology}
    \begin{itemize}
        \item $\xi$ - 'xi'
        \item $\xi_N$ - 'xi n'
        \item $\xi_{[-N, \infty)}$ - 'xi plus inf'
        \item $\xi_{(-\infty, N]}$ - 'xi minus inf'
        \item $[-N, N]$ - 'main interval'
        \item $\xi_N(t) \ne \varnothing$ - 'process is still alive'
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Natural language definition of metastability}

    Recall that a system is metastable if:
    \begin{enumerate}
        \item It stays out of its equilibrium during a memoryless random time
        \item During this time in which the system is out of equilibrium it stabilizes
    \end{enumerate}
    Let's elaborate some more on point 2.
    \begin{enumerate}
        \item Assume that for a given $N$ we have some intermediate timescale such $R_N$ that $R_N << \beta_N$
        \item Say we measure a temporal mean of some observable quantity of a system (e.g. particle density) over this timescale
        \item We say system has stabilized if this mean is close to the expectation of this observable quantity w.r.t. some fixed probability distribution on $\{0,1\}^{\mathbb{Z}}$
    \end{enumerate}
\end{frame}

\subsection{Towards the rigorous definition} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks

\begin{frame}
    \frametitle{Towards the rigorous definition}
    To ensure $R_N << \beta_N$, let's require $R_N / \beta_N \rightarrow 0$ as $N \rightarrow \infty$.
    \\~\\ 
    For our purposes, define \textbf{observable quantity of a system} as $f$, such that:
    \begin{itemize}
        \item $f : \{0,1\}^{\mathbb{Z}} \rightarrow \mathbb{R}$
        \item $f$ is local
    \end{itemize}
    Define 
    \[
        \Lambda(f) :=  \text{ smallest } B\subset \mathbb{N} \text{ s.t. } f(A) = f(A\cap B) \ \forall B \subset \mathbb{N}
    \]
    We can think of $\Lambda(f)$ as the ``support'' of $f$.
\end{frame}

\begin{frame}
    \frametitle{Towards the rigorous definition}

    Define \textbf{temporal mean} of observable quantity $f(\xi_N(t))$ as:
    \[
        A^N_R(s, f) := R^{-1}\int_s^{s+R}f(\xi_N(t))dt
    \]
    Where:
    \begin{itemize}
        \item $s$ is the time in which we start our measurement
        \item $R$ is the duration over which we calculate the temporal mean
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item Take our \textbf{fixed probability distribution} to be $\mu$ (i.e. the non-zero invariant measure of the contact process in the supercritical regime).
        \item Then, we can define \textbf{expectation of observable quantity} w.r.t to this probability distribtuion as $\mu(f) := \int fd\mu$ 
        \item Take convergence in probability as how we understand \textbf{closeness}.
    \end{itemize}
\end{frame}

\begin{frame}
    Let's try to work out a 1st draft of the theorem. \\~\\
    We want to have a sequence $R_N$, such that:
    \begin{itemize}
        \item $R_N / \beta_N \rightarrow 0$ as $N \rightarrow \infty$
        \item for all $\varepsilon > 0$ and all observable quantities $f$
        \[
            \mathbb{P}\left[ |A_{R_N}(s, f) - \mu(f)|\right < \varepsilon] \rightarrow 0
        \]
    \end{itemize}

    As $N \rightarrow \infty$. \\~\\
    Are we done now?
    \visible<2->{
        \textbf{No!} How do we choose $s$ (the starting point of temporal mean measurement)?
    }
\end{frame}

\begin{frame}
    Define 
    \[K_N = \max\{k \in \mathbb{N}_0: kR_N < T_N\}\]
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.2]{./img/kn_rn.png}
        \caption{Example of $K_N$}
        \label{fig:kn_rn}
    \end{figure}
    Turns out that even if we choose the worst-case interval for averaging, we can still make it highly probable that our temporal mean is as close to the $\mu(f)$ 
    as we want, given that we're free to increase $N$. 
\end{frame}

\begin{frame}
    In other words, we want to have a sequence $R_N$, such that:
    \begin{itemize}
        \item $R_N / \beta_N \rightarrow 0$ as $N \rightarrow \infty$
        \item for all $\varepsilon > 0$ and all observable quantities $f$
        \[
            \mathbb{P}\left[ \max_{\mathbb{N}_0 \ni k < K_N}|A_{R_N}(kR_N, f) - \mu(f)| < \varepsilon\right] \rightarrow 0
        \]
    \end{itemize}

    As $N \rightarrow \infty$. \\~\\
    This draft is almost ready - we now only have one more technicality left.
\end{frame}

\begin{frame}
    We need to choose $L(\varepsilon, f) \in \mathbb{N}$, and we need to have that
    \begin{itemize}
        \item $L < N$
        \item $\Lambda(f) \subset [-N+L, N-L]$ (call $[-N+L, N-L]$ \textbf{inside region})
    \end{itemize}
    ~\\
     Notice that $L$ does not depend on $N$. Thus, having to choose this $L$ doesn't restrict our choice of $f$ - it merely sets the minimum
     $N$ we can consider and we chose to grow $N \rightarrow \infty$.
    \\~\\
    \visible<2->{
        Additionally, define
        \begin{itemize}
            \item $[-N, -N+L)$ - \textbf{left boundary region}
            \item $(N-L, N]$ - \textbf{right boundary region}
        \end{itemize}
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.15]{./img/regions_df.png}
            \caption{Partition of the main interval}
            \label{fig:regions}
        \end{figure}
    }
\end{frame}

\subsection{Statement of the theorem} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks

\begin{frame}
    \frametitle{Theorem 2}

    We're finally ready to formulate the theorem.

    \begin{theorem}[Thermalization]
        If $\lambda > \lambda^*$ there is a sequence $\{R_N\}_{N \in \mathbb{N}} \subset \mathbb{R_+}$ such that:
        \begin{itemize}
            \item $R_N/\beta_N \rightarrow 0$ as $N\rightarrow \infty$
            \item For all $\varepsilon > 0$ and observable quantities $f$ $\exists L(\varepsilon, f) \in \mathbb{N}$ such that
                  \[
                      \mathbb{P}\left[ \max_{\mathbb{N}_0 \ni k < K_N}|A_{R_N}(kR_N, f) - \mu(f)| > \varepsilon\right] \rightarrow 0
                  \]
                  as $N \rightarrow \infty$, where $K_N = \max\{k \in \mathbb{N}_0: kR_N < T_N\}$ and $\Lambda(f) \subset [-N + L, N - L] \cap \mathbb{Z}$
        \end{itemize}
    \end{theorem}
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Set 
    \[B^N_k = \left\{ |A_{R_N}(kR_N, f) - \mu(f)| > \varepsilon\right\}\]
    On $B^N_k$ the difference between temporal mean of observable measured over the $k$-th interval and its expectation is greater than we'd like to.
    $B^N_k$~means \textbf{failure} (in $k$-th interval).
    \\~\\
    We want the probability of having no failures to go to 1
    \[\mathbb{P}\left[ \max_{\mathbb{N}_0 \ni k < K_N}|A_{R_N}(kR_N, f) - \mu(f)| \leq \varepsilon   \right] =
    \mathbb{P}\left[ \bigcap_{\mathbb{N}_0 \ni k < K_N}\left(B^N_k\right)^C \right] \rightarrow 1 \]
    as $N\rightarrow\infty$
\end{frame}

\begin{frame}
    \frametitle{Proof}
    It is easy to show that $\mathbb{P}(K_N = 0) \rightarrow 0$. Thus, we can safely focus only on the subset of $\Omega$ where $K_N \geq 1$. \\~\\
    Combining this with the event from the previous slide and applying some simple algebra, we arrive at the following bound
    \small
    \begin{align*}
        \mathbb{P}\left[ K_N \geq 1,  \bigcap_{\mathbb{N}_0 \ni k < K_N}\left(B^N_k\right)^C \right]
        \geq  \mathbb{P}[1 \leq K_N \leq m] - m^2\max_{1 \leq j}\max_{0 \leq k < j}\mathbb{P}\left[ B^N_k, K_{N} = j \right]
    \end{align*}
    \normalsize
    Our objective will be to find a sequence $\{m_N\}_{N\in\mathbb{N}}$ such that:
    \begin{itemize}
        \item $\mathbb{P}[1 \leq K_N \leq m_N] \rightarrow 1$
        \item $m_N^2\max_{1 \leq j}\max_{0 \leq k < j}\mathbb{P}\left[ B^N_k, K_{N} = j \right] \rightarrow 0$ 
    \end{itemize}
    As $N \rightarrow \infty$
\end{frame}

\begin{frame}
    \frametitle{Proof}
    We will start with the second term. However, first we need some intermediate results. We will say $\xi_N(t)$ is \textbf{wide} at $t$ if 
    it intersects both boundary regions. In other words:
    \[
        \min\xi_N(t) < -N + L \land \max\xi_N(t) > N - L
    \]
    We will call the process \textbf{narrow} otherwise.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.15]{./img/wide_process.png}
        \caption{Snapshot at $t$ of a process wide at $t$}
        \label{fig:wide_process}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.15]{./img/narrow_proces.png}
        \caption{Snapshot at $t$ of a process narrow at $t$}
        \label{fig:narrow_process}
    \end{figure}

\end{frame}
\begin{frame}
    \frametitle{Proof}
    \begin{lemma}[Shielding by a wide process]
        If $\xi_N(t)$ is wide at $t$, then \[\xi_N(t) = \xi(t)\text{ on }[-N + L, N - L] \cap \mathbb{Z}\]\\
        In particular we have $f(\xi_N(t)) = f(\xi(t))$
    \end{lemma}
    Why would this be true? \\~\\
\end{frame}

\begin{frame}
    Essentially, rightmost and leftmost infected individuals ``shield'' the entire space between them from outside influence.
    GIF SHOWING WHAT I MEAN
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Define \[h_L(\eta) = I_{\{\xi : \xi \cup [-N, -N+L] = \varnothing\}}\]
    Notice that $h_L$ can tell us whether a process is wide or not.
    If we take $S$ to be a horizontal flip operator, then $h_L(\xi_N(t))h_L(S\xi_N(t))$ is the desired indicator.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.15]{./img/narrow_flip_ex_1.png}
        \caption{$h_L(\xi_N(t)) = 1$}
        \label{fig:narrow_flip_1}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.15]{./img/narrow_flip_ex_2.png}
        \caption{$h_L(S\xi_N(t)) = 0$}
        \label{fig:narrow_flip_2}
    \end{figure}

\end{frame}

\begin{frame}
    \begin{lemma}[Shielding of the left boundary region]
        \[\{T_N > t\} \subset \{h_L(\xi_N(t)) = h_L(\xi_{[-N, \infty)}(t))\}\]
    \end{lemma}
    In other words, if $\xi_N(t)$ is still alive, it fully determines whether $\xi_{[-N, \infty)}(t)$ intersects the left boundary region.
    \\~\\
    Why would this be true?
    \begin{itemize}
        \item If $\xi_N(t)$ intersects left boundary region, $\xi_{[-N, \infty)}$ intersects it too
        \item If $\xi_N(t)$ does not intersect the left boundary region, but has at least one node still alive, this node shields the left boundary region from outside influence.
            Moreover, no influence can propagate from $-N$. Hence, they need to agree on the left boundary region.
    \end{itemize}
\end{frame}

\begin{frame}
    GIF
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Now we're ready to proceed. Recall, we want

    \begin{itemize}
        \item $m_N^2\max_{1 \leq j}\max_{0 \leq k < j}\mathbb{P}\left[ B^N_k, K_{N} = j \right] \rightarrow 0$ 
    \end{itemize}

    Let's take a closer look at $\mathbb{P}\left[ B^N_k, K_{N} = j \right]$. We will estimate the difference between the temporal average of $f(\xi_N)$ and it's expectation with 
    a triangle inequality.
    For $k < j$ we have
    \begin{gather*}
        \mathbb{P}\left[  \left|A_{R_N}(kR_N, f) - \mu(f)\right| > \varepsilon \right] \leq \\
        \mathbb{P}\left[ \left|R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi(t))dt - \mu(f)\right| > \varepsilon/2 \ \ \lor \right.
        \\
        \left.\left|R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi_N(t)) - f(\xi(t))dt \right| > \varepsilon/2 \right]
    \end{gather*}

\end{frame}

\begin{frame}
    \frametitle{Proof}
    Let's take the first term in the alternative and try to find a bound for
    \[
        \mathbb{P}\left[ \left|R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi(t))dt - \mu(f)\right| > \varepsilon/2 \right]
    \]
    It looks like we could almost apply weak convergence here, but we there's something missing\dots \\~\\ 
    By weak convergence, as $R \rightarrow \infty$, we have
    \[
        R_N^{-1}\int_{kR_N}^{(k+1)R_N}\mathbb{E}\left[ f(\xi(t)) \right]dt \rightarrow \mu(f)
    \]
    \visible<2->{
        If we can now find a way to bind what's below, we're in business! Notice that this is an exceedance probability for a random variable\dots Could Chebyshev be of help here?
        \[
            \mathbb{P}\left[\left|R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi(t)) - \mathbb{E}\left[ f(\xi(t))\right]dt \right| > \varepsilon/4 \right]
        \]
    }
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Indeed, we can find a bound on variance, and thus, by Chebyshev, on exceedance probability. This has to do with the fact that time correlations of $\xi(t)$ decay exponentially fast:
    \begin{theorem}[Exponentially decaying correlations]
        For any $f : \{0,1\}^\mathbb{Z}\rightarrow \mathbb{R}$ local, there are constants $C, \gamma$ such that 
        \[
            \left|\mathbb{C}ov\left[ f(\xi(t)), f(\xi(s))\right]\right| \leq Ce^{-\gamma|s-t|}
        \]
    \end{theorem}
    \visible<2->{
        Now, consider the variance of the expression from the previous slide:
        \[
            \mathbb{E}\left[  \left(R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi(t)) - \mathbb{E}\left[ f(\xi(t)) \right]dt \right)^2 \right]
        \]
    }
    \begin{itemize}
        \item<3-> Looks very much like we could get an expression for a covariance from this! 
        \item<4-> Then, Chebyshev will finish out our job for us.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Recall, we've had:
    \begin{gather*}
        \mathbb{P}\left[ B^N_k, K_{N} = j \right] \leq \mathbb{P}\left[\left|R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi(t))dt - \mu(f)\right| > \varepsilon/2 \right] + \\
        \mathbb{P}\left[\left|R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi_N(t)) - f(\xi(t))dt \right| > \varepsilon/2\right]
    \end{gather*}
    We managed to bind the 1st term using:
    \begin{itemize}
        \item Weak convergence
        \item Exponentially decaying correlations + Chebyshev
    \end{itemize}

    Will this go through for the 2nd term?
    \begin{itemize}
        \item Weak convergence is a no-go. $\xi_N(t)$ converges weakly to $\delta_0$, but $\xi(t)$ converges to $\mu$\dots
        \item However, we know that in our intermediate timescale $\xi_N(t)$ is distributed close to $\mu$, so we should be able, in principle, to find a bound for the 2nd term 
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Wait, didn't we talk about a situation where $f(\xi_N(t))$ is actually equal to $f(\xi(t))$ before?

    \[
        \left|R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi_N(t)) - f(\xi(t))dt \right| > \varepsilon/2 
    \]

    Recall that 
    \begin{itemize}
        \item If $\xi_N$ is wide at $t$, we have $f(\xi_N(t)) = f(\xi(t))$ (shielding by a wide process). 
        \item We can use $h_L$ to tell whether a process is wide or not (flipping trick).
        \item $\xi_N$ is alive for all $t$ we're considering. Hence $h_L(\xi_N(t)) = h_L(\xi_{[-N,\infty)}(t))$ (shielding of the left boundary region).
    \end{itemize}
    We get that the condition above implies

    \[
        2\norm{f}R_N^{-1}\int_{kR_N}^{(k+1)R_N}h_L(\xi_{[-N, \infty)}(t)) + h_L(S\xi_{(-\infty, N]}(t))dt > \varepsilon/2
    \]
\end{frame}


\begin{frame}
    \frametitle{Proof}
    By symmetry,
    \begin{gather*}
        \mathbb{P}\left[  2\norm{f}R_N^{-1}\int_{kR_N}^{(k+1)R_N}h_L(\xi_{[-N, \infty)}(t)) + h_L(S\xi_{(-\infty, N]}(t))dt > \varepsilon/2 \right] \leq \\
        2\mathbb{P}\left[ 2\norm{f}R_N^{-1}\int_{kR_N}^{(k+1)R_N}h_L(\xi_{[-N, \infty)}(t)) > \varepsilon/4 \right]
    \end{gather*}
    This form is something we could use the previous ``machinery'' (weak convergence + covariance + chebyshev) on, but:
    \begin{itemize}
        \item That method would us to make the time average arbitrarily close to $\mu_{[-N, \infty)}(h_L)$, here we need to make it close to 0\dots
        \item Solution: make $\mu_{[-N, \infty)}(h_L)$ close to 0! We can still vary $L$!
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Set $L$ to be such that 
    \[
        \mu_{[-N, \infty)}\left( \left\{ A : A \cap [0,L] = \varnothing \right\} \right) = 
        \mu_{[0, \infty)}\left( \left\{ A : A \cap [0,L] = \varnothing \right\} \right) \leq \varepsilon/(16\norm{f})
    \]
    Notice how we have used that $\mu_{[-N, \infty)}$ is translation invariant. Defining $L$ in this way makes it independent of $N$, as we wanted.
    Surprising, eh?
\end{frame}

\begin{frame}
    \frametitle{Proof}
    We've arrived at

    \[
        \mathbb{P}\left[ B^N_k, K_N = j \right] \leq \mathbb{P}\left[\Gamma_k^{R_N}\right] + 2\mathbb{P}\left[\tilde{\Gamma}_k^{R_N, L}\right]
    \]

    Where

    \[
        \Gamma_k^{R_N} := \left\{  \left|R_N^{-1}\int_{kR_N}^{(k+1)R_N}f(\xi(t))dt - \mu(f)\right| > \varepsilon/2\right\}
    \]

    \[
        \tilde{\Gamma}_k^{R_N} := \left\{  2\norm{f}R_N^{-1}\int_{kR_N}^{(k+1)R_N}h_L(\xi_{[-N, \infty)}(t)) > \varepsilon/4\right\}
    \]

    Formally, our previous considerations give us (for appropriate $R_N$, $L$)

    \[
        \mathbb{P}\left[ B^N_k, K_N = j \right] \leq \mathbb{P}\left[\Gamma_k^{R_N}\right] + 2\mathbb{P}\left[\tilde{\Gamma}_k^{R_N, L}\right] \leq C/R_N
    \]
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Recall we needed to find $m_N, R_N$ such that 
    \begin{itemize}
        \item $\mathbb{P}[1 \leq K_N \leq m_N] \rightarrow 1$
        \item $m_N^2\max_{1 \leq j}\max_{0 \leq k < j}\mathbb{P}\left[ B^N_k, K_{N} = j \right] \rightarrow 0$
    \end{itemize}
    We can bind
    \begin{itemize}
        \item $\max_{1 \leq j}\max_{0 \leq k < j}\mathbb{P}\left[ B^N_k, K_{N} = j \right] \leq C/R_N$
    \end{itemize}
    ~\\
    Another lemma shows that $N/\beta_N \rightarrow 0$. We can then set 
    \[m_N = \beta_N^{1/5}/N^{1/5}, \ \  R_N = \beta_N^{9/10} N^{1/10}\]
    As our solution.
\end{frame}

\begin{frame}
    \frametitle{Proof}
    Originally, the paper also considers shifted versions of $f$, but I thought this would only introduce more confusing indices. If $\tau_i,\ i\in \mathbb{Z}$ is the shift operator
    we can also prove (using exactly the same techniques) that
          \[
              \mathbb{P}\left[ \max_{i}\max_{\mathbb{N}_0 \ni k < K_N}|A_{R_N}(kR_N, \tau_if) - \mu(f)| > \varepsilon\right] \rightarrow 0
          \]
    We must restrict $i$ to be such that $\Lambda(\tau_if) \subset [-N+L, N-L]$, though.
    \\~\\
    \visible<2->{
        As a corollary, same statements also holds for spatial means of observable quantities
              \[
                  \mathbb{P}\left[ \max_{\mathbb{N}_0 \ni k < K_N}|A_{R_N}(kR_N, \bar{f}) - \mu(f)| > \varepsilon\right] \rightarrow 0
              \]
              Where $\bar{f} = \frac{1}{\#i}\sum_i\tau_if$ and $\#i$ is the total number of all $i$s allowed.
    }
\end{frame}

\end{document} 
